{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMGBFxfj/6FNkdvO/jHOPAu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/martinpius/iml_exercise/blob/main/ML_Algorithms.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from timeit import default_timer as timer\n",
        "t1 = timer()\n",
        "try:\n",
        "  from google.colab import drive\n",
        "  drive.mount(\"/content/drive/\", force_remount = True)\n",
        "  import numpy as np\n",
        "  import torch\n",
        "  from sklearn.model_selection import train_test_split\n",
        "  import matplotlib.pyplot as plt\n",
        "  from sklearn.datasets import make_blobs\n",
        "  print(f\">>>> You are in CoLaB with numpy version: {np.__version__}\")\n",
        "except Exception as e:\n",
        "  print(f\">>>> {e}: {e}\\n>>>> Please correct {type(e)} and reload your drive\")\n",
        "\n",
        "def mytimer(t: float = timer())->float:\n",
        "  h = int(t / (60 * 60))\n",
        "  m = int(t % (60 * 60) / 60)\n",
        "  s = int(t % 60)\n",
        "  return f\"hrs: {h}, mins: {m:>02}, secs: {s:>05.2f}\"\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(f\">>>> Available device: {device}\")\n",
        "!nvidia-smi\n",
        "print(f\"\\n>>>> Time elapsed\\t: {mytimer(timer() - t1)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RaOB_pA71YDt",
        "outputId": "01a7eff0-b664-4107-bb5c-1775e9f81b83"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n",
            ">>>> You are in CoLaB with numpy version: 1.22.4\n",
            ">>>> Available device: cpu\n",
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n",
            "\n",
            ">>>> Time elapsed\t: hrs: 0, mins: 00, secs: 31.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We implement various machine learning algorithms from scratch"
      ],
      "metadata": {
        "id": "I1KxUw-rE-dJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NearestNeighborClassifier():\n",
        "  def __init__(self, k):\n",
        "    self.k = k # k is the number of nearest neighbours\n",
        "  \n",
        "  def loader(self, xdata, target):\n",
        "    '''''\n",
        "    Loading the data\n",
        "    '''''\n",
        "    self.xdata = xdata\n",
        "    self.target = target\n",
        "  \n",
        "  def knn_classifier(self, xtest, fast = None):\n",
        "    \"\"\"\"\"\n",
        "    Compute distances for the k-nearest neighbours and return predictions\n",
        "    fast: Optional parameter to either use fast computing or double loop distance \n",
        "    \"\"\"\"\"\n",
        "    if fast == True:\n",
        "      distances = self.compute_fastproxy(xtest)\n",
        "    else:\n",
        "      distances = self.compute_proxy(xtest)\n",
        "    ypreds = self.get_predictions(distances)\n",
        "    return ypreds\n",
        "  \n",
        "  def compute_proxy(self, xtest):\n",
        "    \"\"\"\"\"\n",
        "    Use double loop: Inefficient computing option\n",
        "    \"\"\"\"\"\n",
        "    num_test = xtest.shape[0] # grab the number of exasamples in a test set\n",
        "    num_train = self.xdata.shape[0] # grab number of examples in a train set\n",
        "    distances = np.zeros((num_test, num_train)) # Place-holder matrix with row represent test example and cols associated proxies from train\n",
        "    for i in range(num_test):\n",
        "      for j in range(num_train):\n",
        "        distances[i, j] = np.sqrt(np.sum((xtest[i,:] - self.xdata[j,:])**2)) # Compute the L2 distance\n",
        "    return distances\n",
        "  \n",
        "  def compute_fastproxy(self, xtest):\n",
        "    \"\"\"\"\"\n",
        "    Use single loop : More efficient computing method:\n",
        "    \"\"\"\"\"\n",
        "    num_test = xtest.shape[0]\n",
        "    num_train = self.xdata.shape[0]\n",
        "    distances = np.zeros((num_test, num_train))\n",
        "    for i in range(num_test):\n",
        "      distances[i,:] = np.sqrt(np.sum((self.xdata - xtest[i,:])**2, axis = 1))\n",
        "    return distances\n",
        "  \n",
        "  def get_predictions(self, distances):\n",
        "    \"\"\"\"\"\n",
        "    Use the L2 metric to obtain the most probable class for any new example\n",
        "    \"\"\"\"\"\n",
        "    pred_num = distances.shape[0] # grab total number of predictions\n",
        "    ypreds = np.zeros(pred_num) # Place holder for the predictions\n",
        "    for i in range(pred_num):\n",
        "      pred_indices = np.argsort(distances[i,:]) # returns the indices of the distances of the pred array in ascending order\n",
        "      knn = self.target[pred_indices[: self.k]].astype(int) # Slice the first k nearest neighbours based of the distances\n",
        "      ypreds[i] = np.bincount(knn).argmax() # Fetch the most frequently prediction group/class\n",
        "    return ypreds\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JuG3qUtEFlBP"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  # Loading the data from my google drive\n",
        "  X = np.loadtxt(\"/content/drive/MyDrive/Machine-Learning-Collection-master/ML/algorithms/knn/example_data/data.txt\", delimiter = \",\")\n",
        "  y = np.loadtxt(\"/content/drive/MyDrive/Machine-Learning-Collection-master/ML/algorithms/knn/example_data/targets.txt\")\n",
        "  tic = timer()\n",
        "  myknn = NearestNeighborClassifier(k = 3) # Instantiate with k = 3\n",
        "  myknn.loader(X, y) # Loading the data into our classifier\n",
        "  preds = myknn.knn_classifier(X, fast = True)\n",
        "  acc = np.equal(preds, y).mean()\n",
        "  print(f\">>>> The accuracy score for k = 3 on the training set is: {acc:.2f} %\")\n",
        "  print(f\">>>> Time elapsed: {mytimer(timer() - tic)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gx-5iyBqQM7r",
        "outputId": "34966830-8a1c-4eb3-d345-76466a568874"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>>> The accuracy score for k = 3 on the training set is: 0.93 %\n",
            ">>>> Time elapsed: hrs: 0, mins: 00, secs: 00.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LogReg():\n",
        "  def __init__(self, X, LR = 1e-2, iter = 10000):\n",
        "    self.LR = LR\n",
        "    self.iter = iter\n",
        "    self.m, self.n = X.shape # Unpacking number of examples and features\n",
        "  \n",
        "  def trainer(self, X, y):\n",
        "    # initialize the weights to zeros\n",
        "    self.w = np.zeros((self.n, 1))\n",
        "    self.b = 0\n",
        "    # Iterate over the epochs\n",
        "    for k in range(self.iter + 1):\n",
        "      ypred = self.logits(np.dot(X, self.w) + self.b) # Compute the logits\n",
        "      loss = -1/self.m * np.sum(y * np.log(ypred) + (1 - y)* np.log(1 - ypred))\n",
        "      # compute the gradients of the loss wrt parameters\n",
        "      dw = 1/self.m * np.dot(X.T, (ypred-y))\n",
        "      db = 1/self.m * np.sum(ypred - y)\n",
        "      # performs gradient descent to update the weights\n",
        "      self.w -= dw * self.LR\n",
        "      self.b -= db * self.LR\n",
        "      # Print the loss at every 1000 epochs for monitoring the training\n",
        "      if k % 1000 == 0:\n",
        "        print(f\">>>> The training loss at epoch {k} is: {loss:.4f}\")\n",
        "      # returning the weights to use later for the predictions\n",
        "    return self.w, self.b \n",
        "\n",
        "  def predict(self, xtest):\n",
        "    ypred = self.logits(np.dot(xtest, self.w) + self.b)\n",
        "    pred_labels = ypred > 0.5 # same as np.round(np.sigmoid(ypred))\n",
        "    return pred_labels\n",
        "  \n",
        "  def logits(self, z):\n",
        "    sigm = 1 / (1 + np.exp(-z))\n",
        "    return sigm\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  tic = timer()\n",
        "  print(f\">>>> Loading the toy data from sklearn library\\n\")\n",
        "  X, y = make_blobs(n_samples = 10000, n_features = 2, centers = 2) # Binary classfification problem\n",
        "  y = y[:, np.newaxis] # Reshape the rank1 array into (batch, 1)\n",
        "  print(f\"\\n>>>> Splitting the data into train-test\")\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
        "  print(f\">>>> X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\\\n",
        "  \\n>>>> X_test shape: {X_test.shape}, y_test_shape: {y_test.shape}\")\n",
        "  print(f\"\\n>>>> Training the LogReg classifier:\")\n",
        "  lr = LogReg(X)\n",
        "  lr.trainer(X_train, y_train)\n",
        "  #print(f\"\\n>>>> Computing the predictions\")\n",
        "  pred_labs = lr.predict(X_test)\n",
        "  print(f\"\\n>>>> Compute the accuracy of {lr.__class__.__name__} classifier\")\n",
        "  acc = np.equal(pred_labs, y_test).mean()\n",
        "  print(f\"\\n>>>> The accuracy on the test data: {acc* 100:.2f} %\")\n",
        "  print(f\"\\n>>>> Time elapsed: {mytimer(timer() - tic)}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsInvOgHSEZ6",
        "outputId": "bfc69328-4e2d-4c94-ccf4-88ab2128c43b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>>> Loading the toy data from sklearn library\n",
            "\n",
            "\n",
            ">>>> Splitting the data into train-test\n",
            ">>>> X_train shape: (7000, 2), y_train shape: (7000, 1)  \n",
            ">>>> X_test shape: (3000, 2), y_test_shape: (3000, 1)\n",
            "\n",
            ">>>> Training the LogReg classifier:\n",
            ">>>> The training loss at epoch 0 is: 0.4852\n",
            ">>>> The training loss at epoch 1000 is: 0.0813\n",
            ">>>> The training loss at epoch 2000 is: 0.0632\n",
            ">>>> The training loss at epoch 3000 is: 0.0561\n",
            ">>>> The training loss at epoch 4000 is: 0.0522\n",
            ">>>> The training loss at epoch 5000 is: 0.0496\n",
            ">>>> The training loss at epoch 6000 is: 0.0478\n",
            ">>>> The training loss at epoch 7000 is: 0.0464\n",
            ">>>> The training loss at epoch 8000 is: 0.0453\n",
            ">>>> The training loss at epoch 9000 is: 0.0444\n",
            ">>>> The training loss at epoch 10000 is: 0.0436\n",
            "\n",
            ">>>> Compute the accuracy of LogReg classifier\n",
            "\n",
            ">>>> The accuracy on the test data: 97.47 %\n",
            "\n",
            ">>>> Time elapsed: hrs: 0, mins: 00, secs: 08.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-dXH2CKvv1gC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}